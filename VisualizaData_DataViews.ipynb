{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN67pyAb4OF57IIJ2eM+gdI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thiago-lpereira/2H_ExGIThiago/blob/main/VisualizaData_DataViews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grzUWV1pb_SN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "import warnings\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.initializers import GlorotUniform\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "def plot_training_history(history, dataset_name):\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    ax1.plot(history.history['accuracy'])\n",
        "    ax1.plot(history.history['val_accuracy'])\n",
        "    ax1.set_title(f'Model Accuracy ({dataset_name})')\n",
        "    ax1.set_ylabel('Accuracy')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.legend(['Train', 'Test'], loc='upper left')\n",
        "    ax1.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "    ax2.plot(history.history['loss'])\n",
        "    ax2.plot(history.history['val_loss'])\n",
        "    ax2.set_title(f'Model Loss ({dataset_name})')\n",
        "    ax2.set_ylabel('Loss')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.legend(['Train', 'Test'], loc='upper left')\n",
        "    ax2.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\n--- 1. Starting Breast Cancer Dataset Analysis ---\")\n",
        "\n",
        "cancer_data = load_breast_cancer()\n",
        "X_cancer_df = pd.DataFrame(cancer_data.data, columns=cancer_data.feature_names)\n",
        "y_cancer = cancer_data.target\n",
        "\n",
        "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
        "    X_cancer_df, y_cancer, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "X_train_c_nonscaled = X_train_c.copy()\n",
        "X_test_c_nonscaled = X_test_c.copy()\n",
        "\n",
        "scaler_c = StandardScaler()\n",
        "X_train_c = scaler_c.fit_transform(X_train_c)\n",
        "X_test_c = scaler_c.transform(X_test_c)\n",
        "\n",
        "model_cancer = Sequential([\n",
        "    Dense(16, input_dim=X_train_c.shape[1], activation='relu', kernel_initializer=GlorotUniform(seed=69)),\n",
        "    Dense(8, activation='relu', kernel_initializer=GlorotUniform(seed=69)),\n",
        "    Dense(1, activation='sigmoid', kernel_initializer=GlorotUniform(seed=69))\n",
        "])\n",
        "\n",
        "model_cancer.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history_cancer = model_cancer.fit(\n",
        "    X_train_c, y_train_c,\n",
        "    validation_data=(X_test_c, y_test_c),\n",
        "    epochs=100,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "loss_c, accuracy_c = model_cancer.evaluate(X_test_c, y_test_c, verbose=0)\n",
        "print(f\"Final Test Accuracy (Breast Cancer): {accuracy_c * 100:.2f}%\")\n",
        "\n",
        "plot_training_history(history_cancer, \"Breast Cancer Dataset\")\n",
        "\n",
        "print(\"Serializing Breast Cancer components...\")\n",
        "model_cancer.save('breast_cancer_model.h5')\n",
        "joblib.dump(scaler_c, 'breast_cancer_scaler.pkl')\n",
        "np.savez_compressed(\n",
        "    'breast_cancer_datasets.npz',\n",
        "    X_train=X_train_c_nonscaled.to_numpy(),\n",
        "    X_test=X_test_c_nonscaled.to_numpy(),\n",
        "    y_train=y_train_c,\n",
        "    y_test=y_test_c,\n",
        "    columns=X_train_c_nonscaled.columns.tolist()\n",
        ")\n",
        "print(\"Breast Cancer components serialized.\")\n",
        "\n",
        "\n",
        "print(\"\\n--- 2. Starting Titanic Dataset Analysis ---\")\n",
        "\n",
        "df_titanic = sns.load_dataset('titanic')\n",
        "\n",
        "df_titanic['age'].fillna(df_titanic['age'].median(), inplace=True)\n",
        "df_titanic['embarked'].fillna(df_titanic['embarked'].mode()[0], inplace=True)\n",
        "df_titanic.drop(columns=['deck', 'embark_town', 'alive'], inplace=True)\n",
        "df_titanic['sex'] = df_titanic['sex'].map({'male': 0, 'female': 1})\n",
        "df_titanic = pd.get_dummies(df_titanic, columns=['embarked', 'class', 'who', 'adult_male', 'alone'], drop_first=True, dtype=int)\n",
        "\n",
        "X_titanic = df_titanic.drop('survived', axis=1)\n",
        "y_titanic = df_titanic['survived']\n",
        "\n",
        "X_train_t, X_test_t, y_train_t, y_test_t = train_test_split(\n",
        "    X_titanic, y_titanic, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "X_train_t_nonscaled = X_train_t.copy()\n",
        "X_test_t_nonscaled = X_test_t.copy()\n",
        "\n",
        "scaler_t = StandardScaler()\n",
        "X_train_t = scaler_t.fit_transform(X_train_t)\n",
        "X_test_t = scaler_t.transform(X_test_t)\n",
        "\n",
        "model_titanic = Sequential([\n",
        "    Dense(16, input_dim=X_train_t.shape[1], activation='relu', kernel_initializer=GlorotUniform(seed=469)),\n",
        "    Dense(8, activation='relu', kernel_initializer=GlorotUniform(seed=469)),\n",
        "    Dense(1, activation='sigmoid', kernel_initializer=GlorotUniform(seed=469))\n",
        "])\n",
        "model_titanic.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history_titanic = model_titanic.fit(\n",
        "    X_train_t, y_train_t,\n",
        "    validation_data=(X_test_t, y_test_t),\n",
        "    epochs=100,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "loss_t, accuracy_t = model_titanic.evaluate(X_test_t, y_test_t, verbose=0)\n",
        "print(f\"Final Test Accuracy (Titanic): {accuracy_t * 100:.2f}%\")\n",
        "\n",
        "plot_training_history(history_titanic, \"Titanic Dataset\")\n",
        "\n",
        "print(\"Serializing Titanic components...\")\n",
        "model_titanic.save('titanic_model.h5')\n",
        "joblib.dump(scaler_t, 'titanic_scaler.pkl')\n",
        "np.savez_compressed(\n",
        "    'titanic_datasets.npz',\n",
        "    X_train=X_train_t_nonscaled.to_numpy(),\n",
        "    X_test=X_test_t_nonscaled.to_numpy(),\n",
        "    y_train=y_train_t.to_numpy(),\n",
        "    y_test=y_test_t.to_numpy(),\n",
        "    columns=X_train_t_nonscaled.columns.tolist()\n",
        ")\n",
        "print(\"Titanic components serialized.\")\n",
        "\n",
        "\n",
        "print(\"\\n--- 3. Starting Adult (Income) Dataset Analysis ---\")\n",
        "\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
        "columns = [\n",
        "    'age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status',\n",
        "    'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss',\n",
        "    'hours-per-week', 'native-country', 'income'\n",
        "]\n",
        "df_adult = pd.read_csv(url, header=None, names=columns, na_values=' ?', skipinitialspace=True)\n",
        "\n",
        "df_adult.dropna(inplace=True)\n",
        "\n",
        "df_adult['income'] = df_adult['income'].map({'<=50K': 0, '>50K': 1})\n",
        "\n",
        "df_adult = pd.get_dummies(df_adult, drop_first=True, dtype=int)\n",
        "\n",
        "X_adult = df_adult.drop('income', axis=1)\n",
        "y_adult = df_adult['income']\n",
        "\n",
        "X_train_a, X_test_a, y_train_a, y_test_a = train_test_split(\n",
        "    X_adult, y_adult, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "X_train_a_nonscaled = X_train_a.copy()\n",
        "X_test_a_nonscaled = X_test_a.copy()\n",
        "\n",
        "scaler_a = StandardScaler()\n",
        "X_train_a = scaler_a.fit_transform(X_train_a)\n",
        "X_test_a = scaler_a.transform(X_test_a)\n",
        "\n",
        "model_adult = Sequential([\n",
        "    Dense(32, input_dim=X_train_a.shape[1], activation='relu', kernel_initializer=GlorotUniform(seed=777)),\n",
        "    Dense(16, activation='relu', kernel_initializer=GlorotUniform(seed=777)),\n",
        "    Dense(1, activation='sigmoid', kernel_initializer=GlorotUniform(seed=777))\n",
        "])\n",
        "model_adult.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history_adult = model_adult.fit(\n",
        "    X_train_a, y_train_a,\n",
        "    validation_data=(X_test_a, y_test_a),\n",
        "    epochs=50,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "loss_a, accuracy_a = model_adult.evaluate(X_test_a, y_test_a, verbose=0)\n",
        "print(f\"Final Test Accuracy (Adult Income): {accuracy_a * 100:.2f}%\")\n",
        "\n",
        "plot_training_history(history_adult, \"Adult Income Dataset\")\n",
        "\n",
        "print(\"Serializing Adult Income components...\")\n",
        "model_adult.save('adult_model.h5')\n",
        "joblib.dump(scaler_a, 'adult_scaler.pkl')\n",
        "np.savez_compressed(\n",
        "    'adult_datasets.npz',\n",
        "    X_train=X_train_a_nonscaled.to_numpy(),\n",
        "    X_test=X_test_a_nonscaled.to_numpy(),\n",
        "    y_train=y_train_a.to_numpy(),\n",
        "    y_test=y_test_a.to_numpy(),\n",
        "    columns=X_train_a_nonscaled.columns.tolist()\n",
        ")\n",
        "print(\"Adult Income components serialized.\")\n",
        "\n",
        "\n",
        "# --- 4. Deserialization and Verification ---\n",
        "\n",
        "print(\"\\n\\n--- 4. Starting Deserialization and Verification ---\")\n",
        "\n",
        "print(\"\\n--- Verifying Breast Cancer ---\")\n",
        "loaded_model_c = load_model('breast_cancer_model.h5')\n",
        "loaded_model_c.summary()\n",
        "\n",
        "loaded_scaler_c = joblib.load('breast_cancer_scaler.pkl')\n",
        "data_c = np.load('breast_cancer_datasets.npz', allow_pickle=True)\n",
        "\n",
        "loaded_columns_c = data_c['columns']\n",
        "X_train_loaded_c = pd.DataFrame(data_c['X_train'], columns=loaded_columns_c)\n",
        "X_test_loaded_c = pd.DataFrame(data_c['X_test'], columns=loaded_columns_c)\n",
        "y_train_loaded_c = data_c['y_train']\n",
        "y_test_loaded_c = data_c['y_test']\n",
        "print(f\"DataFrame shape: {X_test_loaded_c.shape}\") # Isso agora deve mostrar (num_rows, 14)\n",
        "\n",
        "\n",
        "X_test_scaled_loaded_c = loaded_scaler_c.transform(X_test_loaded_c)\n",
        "loss_loaded_c, accuracy_loaded_c = loaded_model_c.evaluate(X_test_scaled_loaded_c, y_test_loaded_c, verbose=0)\n",
        "print(f\"Accuracy of LOADED model (Breast Cancer): {accuracy_loaded_c * 100:.2f}%\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "loaded_model = load_model('titanic_model.h5')\n",
        "print(\"Model loaded successfully.\")\n",
        "loaded_model.summary()\n",
        "\n",
        "loaded_scaler = joblib.load('titanic_scaler.pkl')\n",
        "print(\"Scaler loaded successfully.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "data = np.load('titanic_datasets.npz', allow_pickle=True)\n",
        "\n",
        "loaded_columns = data['columns']\n",
        "\n",
        "X_train_loaded = pd.DataFrame(data['X_train'], columns=loaded_columns)\n",
        "X_test_loaded = pd.DataFrame(data['X_test'], columns=loaded_columns)\n",
        "y_train_loaded = data['y_train']\n",
        "y_test_loaded = data['y_test']\n",
        "print(\"Datasets loaded successfully.\")\n",
        "print(f\"DataFrame shape: {X_test_loaded.shape}\") # Isso agora deve mostrar (num_rows, 14)\n",
        "\n",
        "X_test_scaled_loaded = loaded_scaler.transform(X_test_loaded)\n",
        "loss_loaded, accuracy_loaded = loaded_model.evaluate(X_test_scaled_loaded, y_test_loaded, verbose=0)\n",
        "print(f\"Accuracy of the loaded model on the test data: {accuracy_loaded * 100:.2f}%\")\n",
        "\n",
        "\n",
        "print(\"Deserialization and verification complete.\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n--- Verifying Adult Income ---\")\n",
        "loaded_model_a = load_model('adult_model.h5')\n",
        "loaded_model_a.summary()\n",
        "loaded_scaler_a = joblib.load('adult_scaler.pkl')\n",
        "data_a = np.load('adult_datasets.npz', allow_pickle=True)\n",
        "\n",
        "loaded_columns_a = data_a['columns']\n",
        "X_train_loaded_a = pd.DataFrame(data_a['X_train'], columns=loaded_columns_a)\n",
        "X_test_loaded_a = pd.DataFrame(data_a['X_test'], columns=loaded_columns_a)\n",
        "y_train_loaded_a = data_a['y_train']\n",
        "y_test_loaded_a = data_a['y_test']\n",
        "print(f\"DataFrame shape: {X_test_loaded_a.shape}\") # Isso agora deve mostrar (num_rows, 14)\n",
        "\n",
        "X_test_scaled_loaded_a = loaded_scaler_a.transform(X_test_loaded_a)\n",
        "loss_loaded_a, accuracy_loaded_a = loaded_model_a.evaluate(X_test_scaled_loaded_a, y_test_loaded_a, verbose=0)\n",
        "print(f\"Accuracy of LOADED model (Adult Income): {accuracy_loaded_a * 100:.2f}%\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Deserialization and all verifications complete. ---\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OjS0LsGaEpm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import accuracy_score\n",
        "from numpy import mean\n",
        "import joblib\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# %%\n",
        "# Load the Breast Cancer dataset\n",
        "data_cancer = load_breast_cancer()\n",
        "df_cancer = pd.DataFrame(data_cancer.data, columns=data_cancer.feature_names)\n",
        "df_cancer['target'] = data_cancer.target\n",
        "\n",
        "# Split data (though not strictly needed for the sampling loop below, good practice)\n",
        "X_cancer = df_cancer[data_cancer.feature_names]\n",
        "y_cancer = df_cancer['target']\n",
        "\n",
        "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
        "    X_cancer, y_cancer,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "print(\"X_train_c:\", X_train_c.shape)\n",
        "print(\"X_test_c: \", X_test_c.shape)\n",
        "print(\"y_train_c:\", y_train_c.shape)\n",
        "print(\"y_test_c: \", y_test_c.shape)\n",
        "\n",
        "# %%\n",
        "# Load the model and scaler for Breast Cancer\n",
        "modelo_cancer = load_model(\"breast_cancer_model.h5\")\n",
        "scaler_c = joblib.load('breast_cancer_scaler.pkl')\n",
        "\n",
        "# Predict and evaluate on the scaled test data\n",
        "X_test_c_scaled = scaler_c.transform(X_test_c)\n",
        "y_pred_c = modelo_cancer.predict(X_test_c_scaled)\n",
        "\n",
        "# Convert predictions to binary classes\n",
        "y_pred_classes_c = (y_pred_c > 0.5).astype(\"int32\")\n",
        "\n",
        "display(y_pred_c)\n",
        "\n",
        "# %%\n",
        "# Calculate accuracy\n",
        "acertos_c = accuracy_score(y_test_c, y_pred_classes_c)\n",
        "print(\"Acurácia:\", acertos_c)\n",
        "\n",
        "# %%\n",
        "# The following code block is for repeated evaluation on samples of the Breast Cancer dataset\n",
        "breast_cancer_accuracies = []\n",
        "\n",
        "for i in range(0,1000):\n",
        "    # Sample from the original Breast Cancer data\n",
        "    df_sample_c = df_cancer.sample(n=100, random_state=i, replace=False)\n",
        "\n",
        "    # Separate features and target for the sample\n",
        "    X_sample_c = df_sample_c[data_cancer.feature_names]\n",
        "    y_sample_c = df_sample_c['target']\n",
        "\n",
        "    # Scale the sampled data\n",
        "    X_sample_scaled_c = scaler_c.transform(X_sample_c)\n",
        "\n",
        "    # Evaluate the loaded model on the sampled and scaled data\n",
        "    loss_c, accuracy_c = modelo_cancer.evaluate(X_sample_scaled_c, y_sample_c, verbose=0)\n",
        "    breast_cancer_accuracies.append(accuracy_c)\n",
        "\n",
        "\n",
        "# %%\n",
        "print(\"Mean Accuracy (Breast Cancer Samples):\", mean(breast_cancer_accuracies))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NE6H7DiMJN0F",
        "outputId": "eced1308-ef6d-4090-a43c-d46f4086f581"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train_c: (455, 30)\n",
            "X_test_c:  (114, 30)\n",
            "y_train_c: (455,)\n",
            "y_test_c:  (114,)\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[9.82021391e-01],\n",
              "       [7.74852094e-09],\n",
              "       [1.80249008e-05],\n",
              "       [9.99978065e-01],\n",
              "       [9.99998629e-01],\n",
              "       [5.79698894e-17],\n",
              "       [6.79356609e-14],\n",
              "       [3.02863633e-03],\n",
              "       [6.95912480e-01],\n",
              "       [9.99997199e-01],\n",
              "       [8.73546660e-01],\n",
              "       [1.25560269e-04],\n",
              "       [9.99852836e-01],\n",
              "       [4.65241494e-04],\n",
              "       [9.99995768e-01],\n",
              "       [5.00314854e-07],\n",
              "       [9.99985456e-01],\n",
              "       [9.99999702e-01],\n",
              "       [9.99999940e-01],\n",
              "       [4.60664701e-11],\n",
              "       [9.96562779e-01],\n",
              "       [9.99636173e-01],\n",
              "       [1.06882005e-14],\n",
              "       [9.99991715e-01],\n",
              "       [9.99971449e-01],\n",
              "       [9.99997199e-01],\n",
              "       [9.99997020e-01],\n",
              "       [9.99095678e-01],\n",
              "       [9.99925256e-01],\n",
              "       [1.27150968e-09],\n",
              "       [9.99902785e-01],\n",
              "       [9.99998987e-01],\n",
              "       [9.97683465e-01],\n",
              "       [9.99875307e-01],\n",
              "       [9.99993265e-01],\n",
              "       [9.99865949e-01],\n",
              "       [8.51708557e-03],\n",
              "       [9.98818994e-01],\n",
              "       [1.63611826e-08],\n",
              "       [9.93141532e-01],\n",
              "       [9.99989808e-01],\n",
              "       [6.10061852e-06],\n",
              "       [9.99964356e-01],\n",
              "       [9.99981105e-01],\n",
              "       [9.97889161e-01],\n",
              "       [8.48436713e-01],\n",
              "       [9.99996006e-01],\n",
              "       [9.99757767e-01],\n",
              "       [9.92864072e-01],\n",
              "       [9.99930739e-01],\n",
              "       [1.24742376e-08],\n",
              "       [6.58551338e-12],\n",
              "       [7.31352448e-01],\n",
              "       [9.95389819e-01],\n",
              "       [9.99998629e-01],\n",
              "       [9.99909699e-01],\n",
              "       [9.99993265e-01],\n",
              "       [1.28575362e-18],\n",
              "       [3.77389573e-04],\n",
              "       [9.99999225e-01],\n",
              "       [9.99874473e-01],\n",
              "       [1.22224028e-11],\n",
              "       [4.48513670e-14],\n",
              "       [9.90074992e-01],\n",
              "       [9.99978364e-01],\n",
              "       [9.82149601e-01],\n",
              "       [2.39561793e-09],\n",
              "       [2.71215781e-17],\n",
              "       [9.99979913e-01],\n",
              "       [9.99303699e-01],\n",
              "       [5.45290241e-05],\n",
              "       [1.14742907e-05],\n",
              "       [9.99955416e-01],\n",
              "       [1.38110827e-05],\n",
              "       [9.99948800e-01],\n",
              "       [9.99893606e-01],\n",
              "       [9.99203682e-01],\n",
              "       [4.23916548e-01],\n",
              "       [9.99999464e-01],\n",
              "       [9.99670744e-01],\n",
              "       [8.34842194e-06],\n",
              "       [9.99998927e-01],\n",
              "       [2.39231251e-02],\n",
              "       [6.64654218e-14],\n",
              "       [2.21959181e-05],\n",
              "       [1.33587861e-07],\n",
              "       [4.93460384e-10],\n",
              "       [3.59443334e-07],\n",
              "       [9.99995947e-01],\n",
              "       [9.99922693e-01],\n",
              "       [9.99893427e-01],\n",
              "       [2.14095667e-01],\n",
              "       [9.94658589e-01],\n",
              "       [9.99992490e-01],\n",
              "       [9.99988377e-01],\n",
              "       [9.99982119e-01],\n",
              "       [8.90624241e-11],\n",
              "       [1.46266652e-10],\n",
              "       [9.99999285e-01],\n",
              "       [2.82825965e-07],\n",
              "       [6.15471890e-06],\n",
              "       [1.00000000e+00],\n",
              "       [3.79954779e-09],\n",
              "       [5.37010237e-07],\n",
              "       [9.96565998e-01],\n",
              "       [9.36225474e-01],\n",
              "       [9.99543369e-01],\n",
              "       [6.27362025e-15],\n",
              "       [9.97947812e-01],\n",
              "       [9.98657167e-01],\n",
              "       [3.62354808e-06],\n",
              "       [9.99966204e-01],\n",
              "       [9.85954523e-01],\n",
              "       [2.04168234e-18]], dtype=float32)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia: 0.9824561403508771\n",
            "Mean Accuracy (Breast Cancer Samples): 0.9965200033187867\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import accuracy_score\n",
        "from numpy import mean\n",
        "import joblib\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "print(\"\\n--- 3. Starting Adult (Income) Dataset Analysis ---\")\n",
        "\n",
        "# Load the Adult Income dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
        "columns = [\n",
        "    'age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status',\n",
        "    'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss',\n",
        "    'hours-per-week', 'native-country', 'income'\n",
        "]\n",
        "original_data_adult = pd.read_csv(url, header=None, names=columns, na_values=' ?', skipinitialspace=True)\n",
        "\n",
        "# Define preprocessing function for Adult Income data\n",
        "def preprocess_adult_data(df):\n",
        "    df.dropna(inplace=True)\n",
        "    df['income'] = df['income'].map({'<=50K': 0, '>50K': 1})\n",
        "    df = pd.get_dummies(df, drop_first=True, dtype=int)\n",
        "    return df\n",
        "\n",
        "# Apply preprocessing to the original Adult Income data for train/test split and sampling\n",
        "df_adult_processed = preprocess_adult_data(original_data_adult.copy())\n",
        "\n",
        "X_adult = df_adult_processed.drop('income', axis=1)\n",
        "y_adult = df_adult_processed['income']\n",
        "\n",
        "X_train_a, X_test_a, y_train_a, y_test_a = train_test_split(\n",
        "    X_adult, y_adult, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Fit scaler on the training data\n",
        "scaler_a = StandardScaler()\n",
        "X_train_a_scaled = scaler_a.fit_transform(X_train_a)\n",
        "X_test_a_scaled = scaler_a.transform(X_test_a)\n",
        "\n",
        "\n",
        "print(\"X_train:\", X_train_a.shape)\n",
        "print(\"X_test: \", X_test_a.shape)\n",
        "print(\"y_train:\", y_train_a.shape)\n",
        "print(\"y_test: \", y_test_a.shape)\n",
        "\n",
        "# %%\n",
        "# Load the model and scaler\n",
        "modelo_adult = load_model(\"adult_model.h5\")\n",
        "scaler_a = joblib.load('adult_scaler.pkl')\n",
        "\n",
        "# Predict and evaluate on the scaled test data\n",
        "y_pred_a = modelo_adult.predict(X_test_a_scaled)\n",
        "\n",
        "# Convert predictions to binary classes\n",
        "y_pred_classes_a = (y_pred_a > 0.5).astype(\"int32\")\n",
        "\n",
        "display(y_pred_a)\n",
        "\n",
        "# %%\n",
        "# Calculate accuracy\n",
        "acertos_a = accuracy_score(y_test_a, y_pred_classes_a)\n",
        "print(\"Acurácia:\", acertos_a)\n",
        "\n",
        "# %%\n",
        "# The following code block is for repeated evaluation on samples of the Adult Income dataset\n",
        "# It's important to use the correctly processed and scaled Adult Income data here as well.\n",
        "\n",
        "adult_accuracies = []\n",
        "\n",
        "for i in range(0,1000):\n",
        "    # Sample from the processed Adult Income data\n",
        "    df_sample_processed_a = df_adult_processed.sample(n=100, random_state=i, replace=False)\n",
        "\n",
        "    # Separate features and target for the sample\n",
        "    X_sample_a = df_sample_processed_a.drop('income', axis=1)\n",
        "    y_sample_a = df_sample_processed_a['income']\n",
        "\n",
        "    # Scale the sampled data using the fitted scaler\n",
        "    X_sample_scaled_a = scaler_a.transform(X_sample_a)\n",
        "\n",
        "    # Evaluate the loaded model on the sampled and scaled data\n",
        "    loss_a, accuracy_a = modelo_adult.evaluate(X_sample_scaled_a, y_sample_a, verbose=0)\n",
        "    adult_accuracies.append(accuracy_a)\n",
        "\n",
        "# %%\n",
        "print(\"Mean Accuracy (Adult Income Samples):\", mean(adult_accuracies))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "XD5MZIdcLZlw",
        "outputId": "a7230416-18b2-4c0a-b3bf-3e17a3ab2c99"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 3. Starting Adult (Income) Dataset Analysis ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train: (26048, 100)\n",
            "X_test:  (6513, 100)\n",
            "y_train: (26048,)\n",
            "y_test:  (6513,)\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[1.7498140e-03],\n",
              "       [2.8722030e-01],\n",
              "       [9.2872447e-01],\n",
              "       ...,\n",
              "       [1.0000000e+00],\n",
              "       [1.5830391e-03],\n",
              "       [2.3615550e-05]], dtype=float32)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia: 0.8489175495163519\n",
            "Mean Accuracy (Adult Income Samples): 0.8786799997091294\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import accuracy_score\n",
        "import seaborn as sns\n",
        "from numpy import mean\n",
        "import joblib\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "print(\"\\n--- 2. Starting Titanic Dataset Analysis ---\")\n",
        "\n",
        "# Load the Titanic dataset\n",
        "original_data_titanic = sns.load_dataset('titanic')\n",
        "\n",
        "# Define preprocessing function for Titanic data\n",
        "def preprocess_titanic_data(df):\n",
        "    df['age'].fillna(df['age'].median(), inplace=True)\n",
        "    df['embarked'].fillna(df['embarked'].mode()[0], inplace=True)\n",
        "    df.drop(columns=['deck', 'embark_town', 'alive'], inplace=True)\n",
        "    df['sex'] = df['sex'].map({'male': 0, 'female': 1})\n",
        "    df = pd.get_dummies(df, columns=['embarked', 'class', 'who', 'adult_male', 'alone'], drop_first=True, dtype=int)\n",
        "    return df\n",
        "\n",
        "# Apply preprocessing to the original Titanic data for train/test split and sampling\n",
        "df_titanic_processed = preprocess_titanic_data(original_data_titanic.copy())\n",
        "\n",
        "X_titanic = df_titanic_processed.drop('survived', axis=1)\n",
        "y_titanic = df_titanic_processed['survived']\n",
        "\n",
        "X_train_t, X_test_t, y_train_t, y_test_t = train_test_split(\n",
        "    X_titanic, y_titanic, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Fit scaler on the training data\n",
        "scaler_t = StandardScaler()\n",
        "X_train_t_scaled = scaler_t.fit_transform(X_train_t)\n",
        "X_test_t_scaled = scaler_t.transform(X_test_t)\n",
        "\n",
        "\n",
        "print(\"X_train:\", X_train_t.shape)\n",
        "print(\"X_test: \", X_test_t.shape)\n",
        "print(\"y_train:\", y_train_t.shape)\n",
        "print(\"y_test: \", y_test_t.shape)\n",
        "\n",
        "# %%\n",
        "# Load the model and scaler for Titanic\n",
        "modelo_titanic = load_model(\"titanic_model.h5\")\n",
        "scaler_t = joblib.load('titanic_scaler.pkl')\n",
        "\n",
        "# Predict and evaluate on the scaled test data\n",
        "y_pred_t = modelo_titanic.predict(X_test_t_scaled)\n",
        "\n",
        "# Convert predictions to binary classes\n",
        "y_pred_classes_t = (y_pred_t > 0.5).astype(\"int32\")\n",
        "\n",
        "display(y_pred_t)\n",
        "\n",
        "# %%\n",
        "# Calculate accuracy\n",
        "acertos_t = accuracy_score(y_test_t, y_pred_classes_t)\n",
        "print(\"Acurácia:\", acertos_t)\n",
        "\n",
        "# %%\n",
        "# The following code block is for repeated evaluation on samples of the Titanic dataset\n",
        "# It's important to use the correctly processed and scaled Titanic data here as well.\n",
        "# Also, the variable for storing accuracies should be used in the mean function.\n",
        "\n",
        "titanic_accuracies = []\n",
        "\n",
        "for i in range(0,1000):\n",
        "    # Sample from the processed Titanic data\n",
        "    df_sample_processed_t = df_titanic_processed.sample(n=100, random_state=i, replace=False)\n",
        "\n",
        "    # Separate features and target for the sample\n",
        "    X_sample_t = df_sample_processed_t.drop('survived', axis=1)\n",
        "    y_sample_t = df_sample_processed_t['survived']\n",
        "\n",
        "    # Scale the sampled data using the fitted scaler\n",
        "    X_sample_scaled_t = scaler_t.transform(X_sample_t)\n",
        "\n",
        "    # Evaluate the loaded model on the sampled and scaled data\n",
        "    loss_t, accuracy_t = modelo_titanic.evaluate(X_sample_scaled_t, y_sample_t, verbose=0)\n",
        "    titanic_accuracies.append(accuracy_t)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# %%\n",
        "print(\"Mean Accuracy (Titanic Samples):\", mean(titanic_accuracies))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "g4wijwFBLfdn",
        "outputId": "503f3dcc-0ae6-48d7-902e-e31a073ece4c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 2. Starting Titanic Dataset Analysis ---\n",
            "X_train: (712, 14)\n",
            "X_test:  (179, 14)\n",
            "y_train: (712,)\n",
            "y_test:  (179,)\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[0.19601594],\n",
              "       [0.08616011],\n",
              "       [0.13387816],\n",
              "       [0.98827624],\n",
              "       [0.5630278 ],\n",
              "       [0.97575647],\n",
              "       [0.7641969 ],\n",
              "       [0.15352222],\n",
              "       [0.72053635],\n",
              "       [0.952948  ],\n",
              "       [0.3222808 ],\n",
              "       [0.0924968 ],\n",
              "       [0.2522168 ],\n",
              "       [0.20488204],\n",
              "       [0.09844542],\n",
              "       [0.9715794 ],\n",
              "       [0.26018956],\n",
              "       [0.76432496],\n",
              "       [0.07669904],\n",
              "       [0.36686745],\n",
              "       [0.13912724],\n",
              "       [0.29651594],\n",
              "       [0.42756182],\n",
              "       [0.13695842],\n",
              "       [0.1577647 ],\n",
              "       [0.11506502],\n",
              "       [0.38174084],\n",
              "       [0.08768396],\n",
              "       [0.15058072],\n",
              "       [0.4225246 ],\n",
              "       [0.13486668],\n",
              "       [0.43889967],\n",
              "       [0.3442332 ],\n",
              "       [0.410728  ],\n",
              "       [0.13810253],\n",
              "       [0.19940655],\n",
              "       [0.2865437 ],\n",
              "       [0.7641969 ],\n",
              "       [0.97326845],\n",
              "       [0.14056674],\n",
              "       [0.07987513],\n",
              "       [0.13526693],\n",
              "       [0.14094977],\n",
              "       [0.09358966],\n",
              "       [0.7253473 ],\n",
              "       [0.09371332],\n",
              "       [0.13554417],\n",
              "       [0.14100845],\n",
              "       [0.1370897 ],\n",
              "       [0.21356127],\n",
              "       [0.73522496],\n",
              "       [0.9598977 ],\n",
              "       [0.01550804],\n",
              "       [0.83566344],\n",
              "       [0.05359785],\n",
              "       [0.949325  ],\n",
              "       [0.08845872],\n",
              "       [0.9898093 ],\n",
              "       [0.9552507 ],\n",
              "       [0.7321305 ],\n",
              "       [0.13483725],\n",
              "       [0.9872077 ],\n",
              "       [0.962914  ],\n",
              "       [0.27932954],\n",
              "       [0.09358966],\n",
              "       [0.955371  ],\n",
              "       [0.15003565],\n",
              "       [0.13995391],\n",
              "       [0.23877898],\n",
              "       [0.98520654],\n",
              "       [0.9799566 ],\n",
              "       [0.9701101 ],\n",
              "       [0.3344065 ],\n",
              "       [0.97730315],\n",
              "       [0.1376815 ],\n",
              "       [0.10715024],\n",
              "       [0.77267456],\n",
              "       [0.95510286],\n",
              "       [0.9503334 ],\n",
              "       [0.3801701 ],\n",
              "       [0.05133133],\n",
              "       [0.9478526 ],\n",
              "       [0.9808566 ],\n",
              "       [0.09357961],\n",
              "       [0.40375924],\n",
              "       [0.5074336 ],\n",
              "       [0.9889046 ],\n",
              "       [0.9788076 ],\n",
              "       [0.27996612],\n",
              "       [0.13897209],\n",
              "       [0.77722436],\n",
              "       [0.09914374],\n",
              "       [0.26326692],\n",
              "       [0.093573  ],\n",
              "       [0.14056675],\n",
              "       [0.14046359],\n",
              "       [0.20157672],\n",
              "       [0.0891645 ],\n",
              "       [0.9495625 ],\n",
              "       [0.1411301 ],\n",
              "       [0.2322298 ],\n",
              "       [0.12372169],\n",
              "       [0.98728174],\n",
              "       [0.11200459],\n",
              "       [0.13954647],\n",
              "       [0.14996637],\n",
              "       [0.98304033],\n",
              "       [0.35669744],\n",
              "       [0.14344653],\n",
              "       [0.38875332],\n",
              "       [0.9782707 ],\n",
              "       [0.13295095],\n",
              "       [0.9873437 ],\n",
              "       [0.3051184 ],\n",
              "       [0.5270086 ],\n",
              "       [0.13845026],\n",
              "       [0.42831755],\n",
              "       [0.12605692],\n",
              "       [0.98125446],\n",
              "       [0.13689569],\n",
              "       [0.04971547],\n",
              "       [0.9818891 ],\n",
              "       [0.9800659 ],\n",
              "       [0.98956144],\n",
              "       [0.13891327],\n",
              "       [0.36668012],\n",
              "       [0.96631175],\n",
              "       [0.41596004],\n",
              "       [0.6194454 ],\n",
              "       [0.13548782],\n",
              "       [0.76440585],\n",
              "       [0.19601594],\n",
              "       [0.07570296],\n",
              "       [0.6242063 ],\n",
              "       [0.5409569 ],\n",
              "       [0.56945693],\n",
              "       [0.9860597 ],\n",
              "       [0.13970684],\n",
              "       [0.09862606],\n",
              "       [0.4544184 ],\n",
              "       [0.13639906],\n",
              "       [0.96786666],\n",
              "       [0.0793208 ],\n",
              "       [0.09794599],\n",
              "       [0.02073459],\n",
              "       [0.9805856 ],\n",
              "       [0.08481126],\n",
              "       [0.11057856],\n",
              "       [0.99059397],\n",
              "       [0.09594049],\n",
              "       [0.2180493 ],\n",
              "       [0.1381605 ],\n",
              "       [0.13743603],\n",
              "       [0.8151014 ],\n",
              "       [0.14094977],\n",
              "       [0.1384813 ],\n",
              "       [0.30736825],\n",
              "       [0.76425755],\n",
              "       [0.96547586],\n",
              "       [0.5484306 ],\n",
              "       [0.23137413],\n",
              "       [0.27985343],\n",
              "       [0.10146897],\n",
              "       [0.9865944 ],\n",
              "       [0.0941594 ],\n",
              "       [0.2530428 ],\n",
              "       [0.06952689],\n",
              "       [0.9782434 ],\n",
              "       [0.13644662],\n",
              "       [0.12355775],\n",
              "       [0.3488138 ],\n",
              "       [0.9874993 ],\n",
              "       [0.30523223],\n",
              "       [0.7632859 ],\n",
              "       [0.1385359 ],\n",
              "       [0.21315216],\n",
              "       [0.03245504],\n",
              "       [0.95838606],\n",
              "       [0.57330596]], dtype=float32)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia: 0.8212290502793296\n",
            "Mean Accuracy (Titanic Samples): 0.8396499986052514\n"
          ]
        }
      ]
    }
  ]
}